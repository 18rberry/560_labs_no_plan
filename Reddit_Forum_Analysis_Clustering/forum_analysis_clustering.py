# -*- coding: utf-8 -*-
"""forum_analysis_clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19XZgOyoJHkUgGuy2kg8PH1bUocGRZY8P
"""

import nltk
import gensim
from nltk.tokenize import word_tokenize
import string
import pandas as pd
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA
import seaborn as sns
from sklearn.manifold import TSNE
from collections import Counter
from wordcloud import WordCloud, STOPWORDS

# download ntlk libraries (run once)
nltk.download('punkt_tab')
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # tokenize
    tokens = word_tokenize(text)
    # Remove stop words
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

df = pd.read_csv('reddit_posts.csv')
df = df.drop_duplicates()
df['title_cleaned'] = df['title'].apply(lambda x: preprocess_text(x))

# additional data cleaning & pre-processing
df = df.dropna(axis=0, how='any')
df = df.reset_index(drop=True)

"""Create Document Embeddings"""

tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(df['title_cleaned'])]

model = Doc2Vec(
    vector_size=50,      # Smaller dimension is better for short titles
    min_count=1,
    epochs=100,          # More training epochs
    dm=0,                # DBOW (distributed bag of words) technique
    window=5,
    workers=4
)

# Build vocab
model.build_vocab(tagged_data)

# Train model
model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)

# Save trained model
model.save("doc2vec_model.bin")

df['doc_embeddings'] = [model.infer_vector(doc.words) for doc in tagged_data]

"""Use Elbow Method to find Optimal K for K-Means Clustering"""

# Get document vectors
doc_vectors = np.array([model.dv[str(i)] for i in range(len(df))])
print(f"Embeddings shape: {doc_vectors.shape}")

inertias = []
K_range = range(2, 11)  # Test K from 2 to 10

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(doc_vectors)
    inertias.append(kmeans.inertia_)

# Plot elbow curve
plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('Number of Clusters: K', fontsize=12)

# plot within cluster sum of squares (aka inertia)
plt.ylabel('Inertia', fontsize=12)
plt.title('Elbow Method For Optimal K', fontsize=14)
plt.grid(True)
plt.xticks(K_range)
plt.show()

"""Based on these results, we see that the ideal number of clusters (or k) is 4. This is using the "elbow" method, which is where the Within Cluster Sum of Squares (WCSS) or inertia decrease slows significantly"""

# perform k-means clustering with optimal k
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['cluster'] = kmeans.fit_predict(doc_vectors)

print(f"Cluster distribution:\n{df['cluster'].value_counts().sort_index()}")

"""Now, we perform Principal Component Analysis (PCA) to reduce the number of dimensions. This reduces our document embeddings to 2 dimensions (2D) so it's easier to plot and visualize the clusters"""

# perform PCA
pca = PCA(n_components=2)
doc_vectors_2d = pca.fit_transform(doc_vectors)

# Add PCA coordinates to dataframe
df['pca_x'] = doc_vectors_2d[:, 0]
df['pca_y'] = doc_vectors_2d[:, 1]

# plot PCA results
plt.figure(figsize=(10, 8))
sns.scatterplot(data=df, x='pca_x', y='pca_y', hue='cluster', palette='viridis', s=100)
plt.title('K-Means Clustering on Doc2Vec Embeddings')
plt.xlabel('PCA Dimension 1')
plt.ylabel('PCA Dimension 2')

# plot centroids from k-means clustering
centroids_2d = pca.transform(kmeans.cluster_centers_)
plt.scatter(centroids_2d[:, 0],
            centroids_2d[:, 1],
            c='red', marker='X', s=300,
            edgecolors='black', linewidths=2, label='Centroids', zorder=5)

plt.legend()
plt.show()

"""T-SNE often gives better 2D visualizations for clustering. Let's create those plots as well:"""

# t-SNE for visualization
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
doc_vectors_tsne = tsne.fit_transform(doc_vectors)
df['tsne_x'] = doc_vectors_tsne[:, 0]
df['tsne_y'] = doc_vectors_tsne[:, 1]

# create plots
plt.figure(figsize=(10, 8))
sns.scatterplot(data=df, x='tsne_x', y='tsne_y', hue='cluster', palette='viridis', s=100)

# Transform centroids to t-SNE space
# approximate bc t-SNE doesn't have a transform method -> visualize cluster regions instead
plt.title('K-Means Clustering with t-SNE Visualization')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.legend()
plt.show()

"""Now, we want to understand the content that makes up each cluster. We want to group similar messages and display messages closet to the centroid of each cluster."""

# find most common words for each cluster
for cluster_id in range(optimal_k):
    print(f"\n{'='*80}")
    print(f"Cluster {cluster_id}")
    print('='*80)

    cluster_posts = df[df['cluster'] == cluster_id]
    print(f"\nNumber of posts: {len(cluster_posts)}")

    # Extract top keywords for each cluster
    all_tokens = []
    for title in cluster_posts['title_cleaned']:
        all_tokens.extend(preprocess_text(str(title)))

    keyword_counts = Counter(all_tokens)
    top_keywords = keyword_counts.most_common(10)
    # filter out 'USC' (all posts are in the USC Reddit)
    top_keywords = [(word, count) for word, count in top_keywords if word != 'usc']

    for word, count in top_keywords:
        print(f"  {word}: {count}")

    # get top reddit labels associated with each cluster
    print("")
    print(f"Top Reddit labels:")

    label_dist = cluster_posts['label'].value_counts().head(3)
    for label, count in label_dist.items():
        pct = (count / len(cluster_posts)) * 100
        print(f"    - {label}: {count} ({pct:.1f}%)")

    print("")
    print(f"Sample Posts for each Cluster (Closest to Centroid):")
    cluster_indices = cluster_posts.index.tolist()
    centroid = kmeans.cluster_centers_[cluster_id]

    # Calculate distances to centroid
    distances = np.linalg.norm(doc_vectors[cluster_indices] - centroid, axis=1)

    # Get 3 closest posts to centroid
    closest_indices = np.argsort(distances)[:5]
    closest_posts = cluster_posts.iloc[closest_indices]

    for idx, (_, row) in enumerate(closest_posts.iterrows(), 1):
        print(f"\n{idx}. {row['title']}")
        print(f"   Score: {row['score']} | Comments: {row['num_comments']}")

"""Now, we can create a WordCloud for each cluster to better understand messages and their keywords on the r/USC reddit thread"""

# generate word cloud for each cluster

for cluster_id in range(optimal_k):
    print(f"\n{'='*80}")
    print(f"Cluster {cluster_id}")
    print('='*80)

    # join all the text content into one string to create word cloud
    cluster_posts = df[df['cluster'] == cluster_id]
    all_tokens = []
    for token_list in cluster_posts['title_cleaned']:
      if token_list not in all_tokens:
        all_tokens.extend(token_list)
    all_words = ' '.join(all_tokens)
    wordcloud = WordCloud(width=800, height=400, background_color='white',
                          max_words=2000, stopwords=stop_words).generate(all_words)

    # plot the word cloud
    plt.figure(figsize=(10, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off") # Turn off the axis values
    plt.show()
